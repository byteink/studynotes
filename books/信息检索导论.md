# 前沿
第一章：倒排索引    
第二章：建立索引之前对文本如何预处理；讨论在不同功能和速度要求下对倒排索引的改进    
第三章：词典搜索结构；处理有拼写错误的查询、与文档集合中的词汇不能精确匹配的处理    
第四章：对大规模文档建立分布式可扩展的倒排索引的算法   
第五章：词典和倒排索引的压缩   
前五张只能处理精确匹配，要么匹配要么不匹配，而不能量化匹配程度、打分等    
第六七章：rank-ordered   
第八章：介绍如何评价一个信息检索系统，可以对不同的系统进行对比   
       
第九章：使用相关性反馈和查询扩展等技术提高返回相关文档的可能性   
第十章：对类似xml、html结构化的标记语言进行检索    
第十一章：概率检索模型    
    
# 第一章 布尔检索
结构化数据如关系数据库；非结构化数据如文本；   
半结构化数据如网页（具有格式标记如标题、段落、注脚）   
信息检索也需要支持半结构化数据，如查找标题中含有java且正文中含有threading的文档。   
    
term是索引的单位。term不一定是word，比如Hong kong就不是一个word。    

倒排记录：    
dictionary（词项） -> posting list 或者叫 invert list （倒排记录表）   
所有的倒排记录一起构成全体倒排记录表（postings）    
dictionary按照字母顺序排序，posting list按照文档ID排序（方便在O(N)时间内求交集）。    
    
多个词项取交集时，可以按照词项的文档频率（即倒排记录表的长度）排序，优先合并两个最短的(并不一定是最优的）   
    
和bool retrieval mode相对的是ranked retrieval models（如vector space model）   
这种模型可以使用 free text queries，比如随意输入一两个单词而不是精确的表达式。    
bool的模型还可以加入 term proximity 操作符，用于限定两个词项在文档中应该相互靠近。    
靠近程度根据两者之间的词个数或者是否同在一个结构单元（在一个句子或段落）来衡量。   

# 第二章 词项字典及倒排记录表
倒排索引建立的步骤：    
收集文档 -> Tokenize -> 对tokens进行语言学处理 -> 按照term进行索引   
    
索引粒度（index granularity）：precision 正确率 和recall 召回率 之间的平衡    
粒度太小：term分布在多个细粒度的小文档里，可能忽略重要的段落，即正确率高，召回率低；    
粒度太大：可能找到很多不相关的匹配。即正确率低，召回率高；   
粒度过大导致的问题也可以通过显式或隐式的临近搜索（proximity search）方法来缓解。   
   
Tokenization：将字符序列转换成一个个token。   
主要任务是确定哪些是正确的token，往往与语言本身有关。   
对于大多数语言特别是一些特定领域的语言来说，往往有一些特定的tokens需要识别为term，比如C++。   
一些特殊类型的字符串序列也需要识别为单独的token，比如email地址、ip地址。    
还有一些不需要索引比如数字、货币量，不然索引词汇量会很大（特例：邮件发送的日期）。    
对于汉字需要先进行分词，另外一种方法是抛弃word-based索引采用短字符序列的方式（例如K-gram）。    
这种方式不用关心token是否跨越词的边界。    
    
停用词：stop word    
某些情况下一些常用词用于匹配的价值不大，需要从文档中去除。这些词成为停用词。    
一种常用的方式是根据term在文档出现的频率排序，选择跟文档主题关系不大的高频词作为停用词。    
不对停用词进行索引有可能对短语查询造成影响。例如文章名为As we may think，如果只针对think进行索引，搜索会很困难。  
    
                                                                     
                                                                               
词项归一化 Normalization：     
查询中的token有可能跟文档中的token不完全一致，例如查询USA希望返回包含U.S.A的文档。     
token normalization就是将看起来不完全一致的tokens归纳为一个等价类。     
最常规的做法是隐式地建立等价类equivalence classes。例如可以使用去掉连字符的映射规则来构建等价类。     
     
另一种建立等价类的方法是维护多个unnormalized tokens之间关联关系，可以进一步扩展为手工构建同义词词表，     
可以对unnormalized token建立索引，查询时对查询词进行扩展 query expansion（节省空间，查询处理耗费时间）；     
也可以在索引构建时就进行扩展建立多个索引（浪费空间）。     
     
隐式建立等价类或者查询扩展的使用程度是一个开放的问题，过度使用很容易无意间造成非预期的扩展结果。     
     
归一化会遇到的问题：     
- 重音和变音符号     
- 大小写转换问题  通常全部转换为小写
- 英语中的其他问题  英式美式不同的拼写方式、日期时间和其他对象具有不同的形式
- 其他语言问题   同一个文档内可能有不同的语言
      
        
              
Stemming 和 lemmatization  词干还原和词形归并：    
词干还原：很粗略地去除单词两端词缀的启发式处理。常常也包含去除派生词缀    
         一般情况下合并单词的派生形式。    
词形归并：通常指利用词汇表和词形分析来去除屈折词缀，返回词的原型或词典中的词，返回的结果成为词元（lemma）    
                  通常只合并词元的不同屈折形式。     
这两者往往通过在索引过程中增加插件的方式来实现。     
英语处理中最常用的词干还原算法是Porter算法。     
其他算法如单遍扫描的Lovins算法、较新的Paice/Husk算法。     
词干还原能够提高召回率但是会降低正确率。                        

      
基于跳表的倒排记录表快速合并算法：      
建立索引的时候在posting list上增加 skip pointer。     
适用于索引变化（更新）不太频繁的场景。           
     
       
       
含位置信息的倒排记录表及短语查询：      
- 二元词索引 Bitword indexes：         
      处理短语查询的一种方法是将文档中每个接续词对看成一个短语。          
      二元次索引的概念可以扩展到更长的序列，如果索引中包含变长的词序列通常就成为短语索引 phrase index。             
- 位置信息索引：              
      对于每个词项的倒排记录，存储文档ID时添加额外的位置信息（在该文档里出现的位置）             
                              文档ID：（位置1、位置2 ... )          
      处理短语查询合并倒排记录表时，不仅判断两个词项是否同时出现在同一个文档里，           
      还需要检查它们的位置关系是否跟短语查询一致。            
- 混合索引机制               
      对某些查询使用短语索引或二元词索引（高频查询），其他短语查询使用位置索引。            
      因为位置索引效率需要合并倒排记录表，效率较低。             
      更复杂的混合索引机制：next word index                   
     
# 第三章 词典及容错式检索

词典搜索的数据结构：           
首要任务是确定每个查询词项是否在词汇表中，两大类解决方案：哈希表和搜索树             
哈希表难以处理词项存在轻微变形的情况，特别地难以处理前缀式查询。          
搜索树的方式可以很好地支持前缀式查询，与哈希表不同，搜索树要求数中的字符集有预定义的排序方式（比如英语A-Z）          
          
通配符查询：          
通配符仅在查询字符串末尾出现一次（如mon\*）称为尾通配符查询（trailing wildcard query)。           
基于搜索树如B-树的词典结构处理尾通配符查询很方便。          
考虑首通配符查询（leading wildcard query）如 \*mon，引入反向B-树(reverse B-tree)结构，存储逆序后的词项。          
同时使用B-树和反向B-树，就可以处理更一般的单项通配符查询（查询只包含一个通配符）。          
更一般的通配符查询：          
- 轮排索引 permuterm index          
    在字符集中引入一个新的符号$，标识词项结束。比如hello扩展为hello$          
    然后构建一个轮排索引，对扩展词项的每个旋转结果都构造一个指针指向原始词项。          
- k-gram索引          
    轮排索引会引起空间的急剧增长。          
    一个k-gram代表由k个字符组成的序列。用一个特殊字符$来表示标识词项的开始或者结束。          
    例如castle所有3-gram包括$ca、ast、asl、sle和le$。          
    在k-gram索引结构中，其词典有词汇表中所有词项的所有k-gram形式构造，k-gram指向所有包含该k-gram的词项。          
    如 etr-> (beetroot, metric, petrift)          
    为了解决返回非预期结果，引入后过滤的步骤（postfiltering），过滤时只需要做简单的字符串匹配即可。          
              
拼写校正：          
两个基本原则：          
1）对于一个拼写错误的查询，在其可能的正确拼写中，选择距离最近（nearest）的一个。                    
2）邻近度相等或相似时，选择更常见的那个。更常见可以通过统计各词项在文档集中出现的次数来获得。          
主要关注两种拼写校正的方法：词项独立（isolated-term)的校正 和 上下文敏感（context-sensitive）的校正。          
词项独立：不管查询时包含多少个词项，每次只考虑一个词项的校正。          
两种词项独立的校正方法：编辑距离（edit distance) 和 k-gram 重合度方法 （k-gram overlap）          
- 编辑距离          
   给定两个字符串s1和s2，编辑距离定义为将s1转换为s2的最小编辑操作数（edit operation）。          
   编辑操作包括将一个字符插入字符串、从字符串中删除一个字符和替换一个字符。          
   可以进一步扩展，不同的编辑操作具有不同的权重。          
- k-gram overlap          
   利用k-gram索引来查找与查询具有很多公共k-gram的词项。          
上下文敏感的拼写校正：          
每个单词拼写都对但是整个短语是拼写错误的，仍要对每个单词找到可能的拼写正确词。          
          
基于发音的校正技术：          
输入了一个和目标词项发音相似的查询。          
基本思路是对每个词项进行一个语音哈希操作，发音相似的词项都被映射为同一个值。          
统称为soundex算法。          
          
# 第四章 索引构建
基于块的排序索引方法 Blocked sort-based indexing：     
由于内存不足必须使用基于磁盘的外部排序算法（external sorting algorithm),     
该算法的核心要求是在拍寻时尽量减少磁盘随机寻道的次数。     
BSBI（blocked sort-based indexing algorithm）是一种解决办法。     
为了更高效建立索引，我们将词项映射成ID。     
1) 第1步，将文档集分割成几个大小相等的部分;     
2) 第2步，将每个部分的词项ID-文档ID对排序；     
3) 第3步，将中间产生的临时排序结果放到磁盘中；     
4) 第4步，将所有的中间文件合并成最终索引     
          
内存式单遍扫描索引构建方法 single-pass in-memory indexing：          
BSBI具有很好的扩展性，但需要一种词项映射为ID的数据结构，文档规模很大的时候该结构可能无法放入内存。     
SPIMI使用词项而不是其ID，它将每个块的词典写入磁盘，对于下一个块则重新采用新的词典。     
          
分布式索引构建方法：          
MapReduce     

动态索引构建方法：     
最简单的索引更新就是周期性对文档集从头开始进行索引重构。     
如果要求能够及时检索到新文档，那么一种解决方法是同时两个索引：     
一个是大的主索引，另一个小的用于存储新文档的信息的辅助索引（auxiliary index），后者保存在内存里。     
检索时可以同时遍历两个索引并将结果合并。文档的删除记录在一个无效位向量（invalidation bit vector)中，     
在返回结果之间可以利用它过滤掉已删除文档。某篇文档的更新通过先删除后重新插入来实现。     
每当辅助索引变得很大时，就将它合并到主索引中。     
     
# 第六章 文档评分、词项权重计算即向量空间模型
## 参数化索引及域索引 Parametric and zone indexes   
文档包含元数据（metadata），元数据通常包含诸如创建日期、作者等字段（field）信息。     
对每个字段都有一个参数化索引（parametric index），我们可以选择在某个字段上满足需求的文档。     
搜索引擎可以支持对一些有序字段在某个取值范围内搜索。     
     
域（zone）和字段很类似，只是它的内容可以是任意的自由文本，比如文档的标题和摘要。     
我们可以对文档的不同域构建独立的倒排索引。域索引字典的term为（原始term+域）。     
另一种域索引的实现方式是将域放到posting list而不是字典中，可以支持域加权评分（weighted zone scoring)技术。     
     
域加权评分     
给定一个布尔查询 q 和 文档 d，给每个 (q,d) 对计算出一个[0, 1]之间的得分，该得分由每个域上的得分线性组合而成。     
每个域上的得分取布尔值，要么是0,要么是1, 每个域都有一个权重gi，都有域权重总和为1.      
该算法有时也称为排序式布尔检索（ranked boolean retrieval）。     
各个域的权重确定倾向于使用机器学习的方式。     

## 词项频率及权重计算

如果文档或域中词项出现的频率越高，那么该文档或域的得分也越高。        
每个词项都赋予一个权重，取决于该词项在文档中出现的次数，这种权重计算方式成为词项频率（term frequency），记做 tf<sub>t,d</sub> 。        
        
逆文档频率 inverse document frequency：        
原始词项频率的问题是认为所有的词项都是同等重要的，然而某些词项对于相关度计算来说几乎或者很少有区分能力。        
文档频率（document frequency) df<sub>t</sub> 表示出现term t 的所有文档的数目。              
由于df本身比较大，所以通常把它映射到一个较小的取值范围内，设文档数目为N，term t 的 idf 定义为：           
<pre>    idf<sub>t</sub> = log (N/df<sub>t</sub>)</pre>          
因此一个罕见词的idf往往很高，而高频词的idf就可能比较低。        

tf-idf权重计算：        
将 tf 和 idf 组合在一起形成最终的权重，tf-idf权重机制对文档 d 中的词项 t 赋予的权重如下：        
<pre>   tf-idf<sub>t,d</sub> = tf<sub>t,d</sub> × idf<sub>t</sub>   </pre>    
重合度评分指标（overlap score measure）：具体对于某个查询，文档 d 的得分是所有查询词项在 d 中出现次数 tf 之和。        
可以对这种方法进行修正，不采用 tf，采用 tf-idf 求和。        
      
